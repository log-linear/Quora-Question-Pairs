{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtS9bAdXL1Eq",
    "pycharm": {}
   },
   "source": [
    "# Project 1: Quora Question Pairs\n",
    "\n",
    "## Description:\n",
    "\n",
    "This notebook uses NLP to generate predictions for the Quora Question Pairs dataset from https://www.kaggle.com/c/quora-question-pairs/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "fwDWrY9pL1Ew",
    "outputId": "a59e5d64-7d91-4e94-e4f6-8317c64c6c5c",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import io\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsf7HQ1UL1E9",
    "pycharm": {}
   },
   "source": [
    "## Function definitions, Training Set Import, Preprocessing\n",
    "\n",
    "### Define helper functions to calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQpD_RUeL1FA",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse(nlp, docs):\n",
    "    parsed_docs = []\n",
    "    \n",
    "    for doc in nlp.pipe(list(docs), n_threads=10):\n",
    "        parsed_docs.append(doc)\n",
    "    \n",
    "    return parsed_docs\n",
    "\n",
    "\n",
    "def get_similarity(docs):    \n",
    "    return docs[0].similarity(docs[1])\n",
    "\n",
    "\n",
    "def get_sentiment(doc):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarity = sid.polarity_scores(doc)\n",
    "\n",
    "    compound = polarity['compound']\n",
    "    neg = polarity['neg']\n",
    "    neu = polarity['neu']\n",
    "    pos = polarity['pos']\n",
    "        \n",
    "    return compound, neg, neu, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4B4bqBxL1FJ",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load in train.csv. For faster computation, only load 2.5% of the full sample, or about 10,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ExDjWTeXL1FM",
    "outputId": "efb8f03e-4ed6-40f5-bf24-cd2d8ee261b3",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "csv = Path.cwd().joinpath('train.csv')\n",
    "p = 0.025\n",
    "df = pd.read_csv(csv,\n",
    "                 index_col='id',\n",
    "                 skiprows=lambda i: i>0 and random.random() > p)\n",
    "df['is_duplicate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpMZF2SeL1FZ",
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Calculate cosine similarity between question 1 and question 2, then concatenate the questions for TFIDF generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdDaEtYpL1Fb",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "df['q1_parsed'] = parse(nlp, df['question1'].astype(str))\n",
    "df['q2_parsed'] = parse(nlp, df['question2'].astype(str))\n",
    "\n",
    "df['similarity'] = df[['q1_parsed', 'q2_parsed']].apply(get_similarity, axis=1)\n",
    "df['q_concat'] = df['question1'].map(str) + ' ' + df['question2']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cEzOu2gL1Fl",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "### Calculate polarity scores for each question separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06xyTLDvL1Fo",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[['compound1', 'neg1', 'neu1', 'pos1']] = df['question1'].apply(\n",
    "    get_sentiment, axis=1, result_type='expand'\n",
    ")\n",
    "df[['compound2', 'neg2', 'neu2', 'pos2']] = df['question1'].apply(\n",
    "    get_sentiment, axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "print(sentiment1.head())\n",
    "print(sentiment2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wtmsa5NvL1F0",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "### Calculate absolute differences in sentimentality for each question-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpBcA5jtL1F5",
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['compound_diff'] = (df['compound1'] - df['compound2']).abs()\n",
    "df['neg_diff'] = (df['neg1'] - df['neg2']).abs()\n",
    "df['neu_diff'] = (df['neu1'] - df['neu2']).abs()\n",
    "df['pos_diff'] = (df['pos1'] - df['pos2']).abs()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eiff4wejL1GD",
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUQTxS0uL1GI",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x = df.drop(['question1', \n",
    "             'question2', \n",
    "             'qid1', \n",
    "             'qid2',\n",
    "             'compound1',\n",
    "             'neu1',\n",
    "             'neg1',\n",
    "             'pos1',\n",
    "             'compound2',\n",
    "             'neu2',\n",
    "             'neg2',\n",
    "             'pos2',\n",
    "             'is_duplicate'], axis=1)\n",
    "y = df['is_duplicate']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCA60B-xL1GV",
    "pycharm": {}
   },
   "source": [
    "## TF-IDF Vectorizer\n",
    "\n",
    "Generate TF-IDF's for the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZsA3STgL1GX",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_tfidf = vectorizer.fit_transform(\n",
    "        x_train['q_concat'].values.astype('U')\n",
    "    )\n",
    "test_tfidf = vectorizer.transform(\n",
    "        x_test['q_concat'].values.astype('U')\n",
    "    )\n",
    "x_train_bow = pd.merge(\n",
    "        x_train.drop('q_concat', axis=1), \n",
    "        pd.DataFrame(train_tfidf.todense(), index=x_train.index), \n",
    "        on=x_train.index\n",
    "    ).set_index('key_0')\n",
    "x_test_bow = pd.merge(\n",
    "        x_test.drop('q_concat', axis=1), \n",
    "        pd.DataFrame(test_tfidf.todense(), index=x_test.index), \n",
    "        on=x_test.index\n",
    "    ).set_index('key_0')\n",
    "\n",
    "x_train_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8jbe8R0L1Ge",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCIrUtjFL1Gh",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='liblinear', random_state=42)\n",
    "logit.fit(x_train_bow, y_train)\n",
    "preds = logit.predict(x_test_bow)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOcECX3iL1Gq",
    "pycharm": {}
   },
   "source": [
    "## Model 2: Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes shows a strong bias towards non-duplicate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyFb_Q7zL1Gs",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train_bow, y_train)\n",
    "preds = mnb.predict(x_test_bow)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEI4VnIlL1G1",
    "pycharm": {}
   },
   "source": [
    "## Feature transformation: Singular Value Decomposition\n",
    "\n",
    "Using sklearn's TruncatedSVD class, reduce the TF-IDF's into a lower feature space of 100 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_lj7SK7L1G2",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "train_tfidf_lsa = svd.fit_transform(train_tfidf)\n",
    "test_tfidf_lsa = svd.transform(test_tfidf)\n",
    "\n",
    "x_train_lsa = pd.merge(\n",
    "        x_train.drop('q_concat', axis=1), \n",
    "        pd.DataFrame(train_tfidf_lsa, index=x_train.index), \n",
    "        on=x_train.index\n",
    "    ).set_index('key_0')\n",
    "x_test_lsa = pd.merge(\n",
    "        x_test.drop('q_concat', axis=1), \n",
    "        pd.DataFrame(test_tfidf_lsa, index=x_test.index), \n",
    "        on=x_test.index\n",
    "    ).set_index('key_0')\n",
    "\n",
    "x_train_lsa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzOdyykFL1HC",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Model 1: Logistic Regression\n",
    "\n",
    "Not much improvement over the non-reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjWcMZMVL1HE",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=999999, solver='liblinear', random_state=42)\n",
    "logit.fit(x_train_lsa, y_train)\n",
    "preds = logit.predict(x_test_lsa)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3lRHPLwL1HQ",
    "pycharm": {}
   },
   "source": [
    "## Model 2: Support Vector Machine\n",
    "\n",
    "Using cosine similarity, sentiment differences, and the decomposed TF-IDF's as features, the linear Support Vector Machine Classifier demonstrates greatly improved performance over Multinomial Naive Bayes, with much less bias toward non-duplicate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUfgKSJZL1HT",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(x_train_lsa, y_train)\n",
    "preds = svc.predict(x_test_lsa)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DFoDI8IbDfn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
